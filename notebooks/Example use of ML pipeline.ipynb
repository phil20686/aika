{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d19234a7",
   "metadata": {},
   "source": [
    "# Example of building an ml pipeline for return prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b040114",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from aika import putki\n",
    "from aika.putki import CalendarChecker, IrregularChecker\n",
    "from aika.putki.context import Defaults, GraphContext\n",
    "from aika.putki.graph import Graph, TaskModule\n",
    "from aika.putki.runners import LocalRunner\n",
    "from aika.putki.interface import Dependency\n",
    "from aika.time.calendars import TimeOfDayCalendar, OffsetCalendar\n",
    "from aika.time.time_of_day import TimeOfDay\n",
    "from aika.time.time_range import TimeRange#\n",
    "from aika.time.timestamp import Timestamp\n",
    "from aika.utilities.fin.macd import macd, ewm_volatility\n",
    "from aika.utilities.fin.returns import arithmetic_bar_returns\n",
    "from aika.ml.generators.walkforward import CausalDataSetGenerator\n",
    "from aika.ml.interface import Pipeline, SklearnEstimator, UnivariateStatelessTransformer, BivariateDataSet, apply_trained_models\n",
    "\n",
    "from aika.datagraph.persistence.hash_backed import HashBackedPersistanceEngine\n",
    "from aika.datagraph.persistence.mongo_backed import MongoBackedPersistanceEngine\n",
    "from pandas_datareader import data\n",
    "import typing as t\n",
    "from pandas.tseries.offsets import BDay\n",
    "import pymongo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8a988aa",
   "metadata": {},
   "source": [
    "## Preamble\n",
    "Hopefully if you have been following the other tutorials, the below should now be familiar. We just create a few tasks that will acquire and produce the data that we want."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27889b1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "engine = HashBackedPersistanceEngine()\n",
    "context = GraphContext(\n",
    "    defaults=Defaults(\n",
    "        version=\"research\", \n",
    "        persistence_engine=engine, \n",
    "        time_range= TimeRange(\"2010\", \"2020\")\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8875d2e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pull_google_finance_data(\n",
    "    tickers : t.List,\n",
    "    time_range,\n",
    "):\n",
    "    df = data.DataReader(list(tickers), \"yahoo\", start=time_range.start, end=time_range.end)\n",
    "    df.index.name = None\n",
    "    df.index = df.index.map(Timestamp) # this ensures it has a timezone.\n",
    "    return df[\"Adj Close\"]\n",
    "\n",
    "close_prices = context.time_series_task(\n",
    "    \"close_prices\",\n",
    "    pull_google_finance_data,\n",
    "    tickers=(\"AAPL\", \"GOOGL\"),\n",
    "    completion_checker=CalendarChecker(\n",
    "        TimeOfDayCalendar(time_of_day=TimeOfDay.from_str(\"00:00 [UTC]\"))\n",
    "    ),\n",
    ")\n",
    "close_prices.run()\n",
    "\n",
    "returns = context.time_series_task(\n",
    "    \"returns\",\n",
    "    arithmetic_bar_returns,\n",
    "    prices=close_prices,\n",
    "    step=1,\n",
    "    time_level=\"end\"\n",
    ")\n",
    "returns.run()\n",
    "returns.read()\n",
    "\n",
    "def risk_adjusted_returns(returns):\n",
    "    return returns.divide(ewm_volatility(returns, span=30).shift(1))\n",
    "\n",
    "risk_adjusted_returns = context.time_series_task(\n",
    "    \"returns.risk_adjusted\",\n",
    "    risk_adjusted_returns,\n",
    "    returns=returns,\n",
    "    time_level=\"end\"\n",
    ")\n",
    "risk_adjusted_returns.run()\n",
    "\n",
    "weekly_returns = context.time_series_task(\n",
    "    \"weekly_returns\",\n",
    "    arithmetic_bar_returns,\n",
    "    prices=close_prices,\n",
    "    step=5,\n",
    "    time_level=\"end\"\n",
    ")\n",
    "weekly_returns.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ed82918",
   "metadata": {},
   "outputs": [],
   "source": [
    "def macd_multi_horizon(\n",
    "    prices : pd.DataFrame,\n",
    "    horizons : t.List[t.Tuple[int, int]],\n",
    "    vol_span : int\n",
    "):\n",
    "    results = []\n",
    "    for fast, slow in horizons:\n",
    "        foo = macd(prices, fast, slow, vol_span)\n",
    "        foo.columns = pd.MultiIndex.from_tuples(\n",
    "            [(name, fast, slow) for name in prices.columns], \n",
    "            names=(\"Symbols\", \"fast\", \"slow\")\n",
    "        )\n",
    "        results.append(foo)\n",
    "    return pd.concat(results, axis=1)\n",
    "\n",
    "all_macd = context.time_series_task(\n",
    "    \"all_macd\",\n",
    "    macd_multi_horizon,\n",
    "    prices=close_prices,\n",
    "    horizons=(\n",
    "        (10,20),\n",
    "        (20,40),\n",
    "        (40,80),\n",
    "        (80,160),\n",
    "        (160,320)\n",
    "    ),\n",
    "    vol_span=90\n",
    ")\n",
    "all_macd.run()\n",
    "all_macd.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f7cf2f7",
   "metadata": {},
   "source": [
    "## Using the pipeline model\n",
    "Here we create a pipeline. It has the followng steps:\n",
    "1. fill na with zeros\n",
    "2. stack the data so time and asset are the index and the columns are the various macd measures.\n",
    "3. Fit a regression from those signals onto the one day ahead future returns (note the use of index_level=\"start\" in the dataset generator, so we index signals (features) to the start of the next bar return (response)).\n",
    "4. Unstack the result.\n",
    "\n",
    "To use this pipeline we firstly need a dataset generator. We use here the causal dataset generator, in effect this will align and then slice the data, this one is set up to provide to the model and expanding series of data, with a minimum period of 300 days and an essentially infinite max, and to step forward by 100 days. So there is a new dataset everytime we have 100 complete days.\n",
    "\n",
    "These are fed into the pipeline so that we get a new model every 100 days.\n",
    "\n",
    "Finally, we go back to the original data and apply the trained models. Trained models become available at point of the last row of data in the training set. So we apply them only after they were causally available. The function \"apply_trained_pipelines\" will take the list of trained models, and data that should be equivalent in form the \"features\" that were given to the dataset generator, and the pipelines transform method will take that data and produce a y. \n",
    "\n",
    "Note that the sklearn estimator will learn the column names when fitting, so that the output y data will have the same column names as the data that it was fitted to. These functions are put into aika tasks so their outputs are persisted and can be inspected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de6bba9f",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "def fill_zeros(df : pd.DataFrame):\n",
    "    return df.fillna(0.0)\n",
    "\n",
    "def stack(df : pd.DataFrame):\n",
    "    return df.stack(level=\"Symbols\")\n",
    "\n",
    "def unstack(df : pd.DataFrame):\n",
    "    return df.unstack(level=\"Symbols\")\n",
    "\n",
    "\n",
    "def fit_model(all_macd : pd.DataFrame, returns : pd.DataFrame):\n",
    "    gen = CausalDataSetGenerator(\n",
    "        features=all_macd,\n",
    "        responses=returns,\n",
    "        step_size=100,\n",
    "        window_size=50000,\n",
    "        min_periods=300,\n",
    "        strict_step_size=True,\n",
    "        causal_kwargs={\n",
    "            \"index_level\":\"start\",\n",
    "            \"contemp\":True\n",
    "        }\n",
    "    )\n",
    "    pipeline = Pipeline(\n",
    "            steps=[\n",
    "                UnivariateStatelessTransformer(fill_zeros),\n",
    "                UnivariateStatelessTransformer(stack),\n",
    "                SklearnEstimator(LinearRegression(fit_intercept=True, copy_X=True)),\n",
    "                UnivariateStatelessTransformer(unstack)\n",
    "            ]\n",
    "        )\n",
    "    return pipeline.apply_to_dataset_generator(gen, time_level=\"end\")\n",
    "\n",
    "\n",
    "fitted_models = context.time_series_task(\n",
    "    \"fitted_models\",\n",
    "    fit_model,\n",
    "    all_macd=all_macd,\n",
    "    returns=risk_adjusted_returns,\n",
    "    completion_checker=IrregularChecker()\n",
    ")\n",
    "fitted_models.run()\n",
    "\n",
    "model_outputs = context.time_series_task(\n",
    "    \"model_outputs\",\n",
    "    apply_trained_models,\n",
    "    models=Dependency(fitted_models, lookback=200 * BDay(), inherit_frequency=False),\n",
    "    data=all_macd,\n",
    "    time_level=0,\n",
    "    contemp=True\n",
    ")\n",
    "model_outputs.run()\n",
    "model_outputs.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a6190c0",
   "metadata": {},
   "source": [
    "## Inspect the Outputs\n",
    "\n",
    "Here we can inspect the coefficients of the regression of the learned model, but looking inside the pipeline. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb594ea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "fitted_models.read().iloc[-1].steps[-2]._estimator.coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f609ed51",
   "metadata": {},
   "source": [
    "Or we can compare the predictions with the actual returns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe3f1d3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "o = model_outputs.read().copy()\n",
    "r = risk_adjusted_returns.read().copy().droplevel(\"end\")\n",
    "\n",
    "o.columns = pd.MultiIndex.from_tuples([(\"Prediction\", symbol) for symbol in o.columns])\n",
    "r.columns = pd.MultiIndex.from_tuples([(\"Returns\", symbol) for symbol in r.columns])\n",
    "\n",
    "results = pd.concat([o,r], axis=1)\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5042435f",
   "metadata": {},
   "source": [
    "Or we can look at eg deciles. Clearly trying to find the optimal way to combine one day ahead momentum signals needs a little more than to use two assets over a reasoanably short time period, so don't spend any time wondering why the results are so feeble!."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a63fb86",
   "metadata": {},
   "outputs": [],
   "source": [
    "foo = results.stack(level=1).dropna()#.pipe(lambda df : df.groupby(df[\"Prediction\"].quantile([x/10 for x in range(10)])).mean())\n",
    "quantiles = foo[\"Prediction\"].quantile([x/10 for x in range(10)])\n",
    "quantiles.index = [x * 10 for x in quantiles.index]\n",
    "grouper = np.searchsorted(quantiles.values, foo[\"Prediction\"].values, side=\"right\")\n",
    "r = foo.groupby(grouper).mean()[\"Returns\"]\n",
    "display(foo.Returns.rolling(10).mean().hist(bins=50))\n",
    "pd.DataFrame({\"p\":quantiles, \"r\": r}).plot.scatter(x=\"p\", y=\"r\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bd4e69f",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "display(foo.corr())\n",
    "foo.plot.scatter(x=\"Prediction\", y=\"Returns\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff486ee4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c72c005",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
